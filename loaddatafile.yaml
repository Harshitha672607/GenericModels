    - name: generic-data-loader
      inputs:
        parameters:
          - name: cdn_url
          - name: target_column
          - name: model_type
          - name: train_split
          - name: shuffle_seed
      outputs:
        artifacts:
          - name: train_data
            path: /tmp/train_data.csv
          - name: test_data
            path: /tmp/test_data.csv
          - name: dataset_info
            path: /tmp/dataset_info.pkl
      container:
        image: python:3.9
        command:
          - sh
          - -c
        args:
          - |
            pip install pandas numpy requests scikit-learn >/dev/null 2>&1
            python -c "
            import os
            import sys
            import io
            import pandas as pd
            import numpy as np
            import requests
            import pickle
            from urllib.parse import unquote
            from sklearn.model_selection import train_test_split

            cdn_url = sys.argv[1]
            target_column = sys.argv[2]
            model_type = sys.argv[3]
            train_split = float(sys.argv[4])
            shuffle_seed = int(sys.argv[5])
            train_data_path = '/tmp/train_data.csv'
            test_data_path = '/tmp/test_data.csv'
            dataset_info_path = '/tmp/dataset_info.pkl'

            print('Generic CSV Loader - Model Type:', model_type)
            print(f'CDN URL: {cdn_url}')
            print(f'Target column: {target_column}')
            print(f'Train split: {train_split}')
            print(f'Shuffle seed: {shuffle_seed}')

            # Validate train_split parameter
            if train_split > 1.0:
                print(f'Warning: train_split {train_split} is greater than 1.0, converting to ratio')
                train_split = train_split / 100.0
                print(f'Converted train_split to: {train_split}')

            decoded_url = unquote(cdn_url)
            try:
                print('Downloading CSV')
                response = requests.get(decoded_url, timeout=30)
                response.raise_for_status()
                df = pd.read_csv(io.BytesIO(response.content))
                print(f'Loaded CSV with shape: {df.shape}')
                print(f'Columns: {list(df.columns)}')
            except Exception as e:
                print(f'Failed to load dataset: {e}')
                raise

            # Determine problem type based on model_type
            if model_type == 'linear_regression':
                problem_type = 'regression'
            else:
                problem_type = 'classification'

            # Check target variable
            target_unique = df[target_column].nunique()
            target_distribution = df[target_column].value_counts()
            print(f'Target variable has {target_unique} unique values: {target_distribution.to_dict()}')

            # Basic data validation
            print('Data Summary:')
            print(df.describe())
            print('Missing values:')
            print(df.isnull().sum())

            # Ensure train_split is between 0 and 1
            train_split = max(0.1, min(0.9, train_split))
            test_size = 1.0 - train_split
            
            print(f'Using train split: {train_split}, test size: {test_size}')

            # Use stratified split for classification if possible
            if problem_type == 'classification':
                min_samples_per_class = df[target_column].value_counts().min()
                can_stratify = min_samples_per_class >= 2
                
                if can_stratify:
                    print('Using stratified split (enough samples in all classes)')
                    train_df, test_df = train_test_split(
                        df, 
                        test_size=test_size, 
                        random_state=shuffle_seed,
                        stratify=df[target_column]
                    )
                else:
                    print(f'Warning: Cannot use stratified split. Minimum samples per class: {min_samples_per_class}')
                    print('Using random split instead')
                    train_df, test_df = train_test_split(
                        df, 
                        test_size=test_size, 
                        random_state=shuffle_seed
                    )
            else:
                # Regression - simple split
                print('Using random split for regression')
                train_df, test_df = train_test_split(
                    df, 
                    test_size=test_size, 
                    random_state=shuffle_seed
                )

            print(f'Train shape: {train_df.shape}, Test shape: {test_df.shape}')
            if problem_type == 'classification':
                print(f'Class distribution in train: {train_df[target_column].value_counts().to_dict()}')
                print(f'Class distribution in test: {test_df[target_column].value_counts().to_dict()}')

            feature_columns = [col for col in df.columns if col != target_column and col != 'employee_id']
            
            # Auto-detect feature types
            numeric_features = []
            categorical_features = []
            for col in feature_columns:
                if col in df.columns:
                    if df[col].dtype in ['int64', 'float64']:
                        numeric_features.append(col)
                    else:
                        categorical_features.append(col)
            
            dataset_info = {
                'total_samples': len(df),
                'train_samples': len(train_df),
                'test_samples': len(test_df),
                'target_column': target_column,
                'feature_columns': feature_columns,
                'numeric_features': numeric_features,
                'categorical_features': categorical_features,
                'train_split_ratio': train_split,
                'shuffle_seed': shuffle_seed,
                'columns': list(df.columns),
                'dtypes': {col: str(df[col].dtype) for col in df.columns},
                'target_distribution': df[target_column].value_counts().to_dict() if problem_type == 'classification' else {},
                'problem_type': problem_type,
                'model_type': model_type,
                'dataset_name': 'employee_performance',
                'used_stratified_split': can_stratify if problem_type == 'classification' else False
            }

            # Save files
            train_df.to_csv(train_data_path, index=False)
            test_df.to_csv(test_data_path, index=False)
            with open(dataset_info_path, 'wb') as f:
                pickle.dump(dataset_info, f)

            print('Dataset processing complete!')
            print(f'Train data saved at: {train_data_path}')
            print(f'Test data saved at: {test_data_path}')
            print(f'Dataset info saved at: {dataset_info_path}')
            print(f'Dataset info keys: {list(dataset_info.keys())}')
            " "{{inputs.parameters.cdn_url}}" "{{inputs.parameters.target_column}}" "{{inputs.parameters.model_type}}" "{{inputs.parameters.train_split}}" "{{inputs.parameters.shuffle_seed}}"
