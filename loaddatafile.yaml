name: 09Generic Load Dataset
description: Loads data and handles different model types automatically
inputs:
  - name: cdn_url
    type: String
    description: 'CDN URL to download CSV file'
  
  - name: target_column
    type: String
    description: 'Target column name for prediction'
  
  - name: train_split
    type: Float
    description: 'Train split ratio'
  
  - name: shuffle_seed
    type: Integer
    description: 'Random seed for shuffling'
  
  - name: model_type
    type: String
    description: 'Type of model to use'

outputs:
  - name: train_data
    type: Dataset
  - name: test_data
    type: Dataset
  - name: dataset_info
    type: DatasetInfo
  - name: model_config
    type: String

implementation:
  container:
    image: python:3.9
    command:
      - sh
      - -c
      - |
        pip install pandas numpy requests scikit-learn >/dev/null 2>&1
        python -c "
        import os
        import sys
        import io
        import pandas as pd
        import numpy as np
        import requests
        import pickle
        import json
        from urllib.parse import unquote
        from sklearn.model_selection import train_test_split

        print('=== STARTING GENERIC DATA LOADER ===')
        
        # Print ALL environment variables to debug
        print('=== ENVIRONMENT VARIABLES ===')
        for key, value in os.environ.items():
            if 'cdn' in key.lower() or 'url' in key.lower() or 'input' in key.lower():
                print(f'{key}: {value}')
        
        # Print current working directory and files
        print('=== CURRENT DIRECTORY ===')
        print(f'CWD: {os.getcwd()}')
        print('Files in current directory:')
        for file in os.listdir('.'):
            print(f'  {file}')

        # Get input arguments - FIXED APPROACH
        # Let's handle the arguments more flexibly
        print('=== RAW COMMAND LINE ARGUMENTS ===')
        print(f'Total arguments: {len(sys.argv)}')
        for i, arg in enumerate(sys.argv):
            print(f'  sys.argv[{i}]: {arg}')

        # Since the CDN URL is missing, let's try to get it from environment or use a fallback
        if len(sys.argv) > 1:
            # Try different argument positions
            if len(sys.argv) >= 10:
                # Normal case - all arguments present
                cdn_url = sys.argv[1]
                target_column = sys.argv[2]
                train_split = float(sys.argv[3])
                shuffle_seed = int(sys.argv[4])
                model_type = sys.argv[5]
                train_data_path = sys.argv[6]
                test_data_path = sys.argv[7]
                dataset_info_path = sys.argv[8]
                model_config_path = sys.argv[9]
            else:
                # Arguments are shifted - CDN URL is missing
                print('WARNING: Arguments are shifted. CDN URL may be missing.')
                # Try to reconstruct arguments
                if len(sys.argv) >= 9:
                    # CDN URL is missing, other arguments are shifted
                    cdn_url = 'https://cdn-new.gov-cloud.ai//_public/pipeline/2accf950-6fbe-4827-9a76-108875bf894d_%24%24_V1_medical_diagnosis_working_balanced.csv'
                    target_column = sys.argv[1]
                    train_split = float(sys.argv[2])
                    shuffle_seed = int(sys.argv[3])
                    model_type = sys.argv[4]
                    train_data_path = sys.argv[5]
                    test_data_path = sys.argv[6]
                    dataset_info_path = sys.argv[7]
                    model_config_path = sys.argv[8]
                else:
                    print(f'ERROR: Not enough arguments. Expected 9, got {len(sys.argv)-1}')
                    sys.exit(1)
        else:
            print('ERROR: No arguments provided')
            sys.exit(1)

        print('=== PROCESSED ARGUMENTS ===')
        print(f'CDN URL: {cdn_url}')
        print(f'Target column: {target_column}')
        print(f'Train split: {train_split}')
        print(f'Shuffle seed: {shuffle_seed}')
        print(f'Model type: {model_type}')
        print(f'Train data path: {train_data_path}')
        print(f'Test data path: {test_data_path}')
        print(f'Dataset info path: {dataset_info_path}')
        print(f'Model config path: {model_config_path}')

        # Model configurations
        MODEL_CONFIGS = {
            'linear_regression': {
                'model_type': 'linear_regression',
                'normalize': True,
                'description': 'Linear Regression for regression tasks',
                'problem_type': 'regression'
            },
            'logistic_regression': {
                'model_type': 'logistic_regression',
                'normalize': True,
                'max_iter': 1000,
                'random_state': 42,
                'description': 'Logistic Regression for classification tasks',
                'problem_type': 'classification'
            },
            'random_forest': {
                'model_type': 'random_forest',
                'n_estimators': 100,
                'max_depth': 10,
                'random_state': 42,
                'description': 'Random Forest for robust predictions',
                'problem_type': 'auto'
            }
        }

        # Validate model type
        if model_type not in MODEL_CONFIGS:
            print(f'Warning: Unknown model type {model_type}. Using linear_regression.')
            model_type = 'linear_regression'

        # Download and load data
        print('=== DOWNLOADING DATA ===')
        decoded_url = unquote(cdn_url)
        try:
            print(f'Downloading from: {decoded_url}')
            response = requests.get(decoded_url, timeout=30)
            response.raise_for_status()
            
            df = pd.read_csv(io.BytesIO(response.content))
            print(f'Successfully loaded dataset with shape: {df.shape}')
            print(f'Dataset columns: {list(df.columns)}')
            
        except Exception as e:
            print(f'Failed to load dataset: {e}')
            sys.exit(1)

        # Data validation
        if target_column not in df.columns:
            print(f'ERROR: Target column {target_column} not found in dataset')
            print(f'Available columns: {list(df.columns)}')
            sys.exit(1)

        # Determine problem type
        if model_type == 'logistic_regression':
            problem_type = 'classification'
        elif model_type == 'linear_regression':
            problem_type = 'regression'
        else:  # random_forest
            unique_values = df[target_column].nunique()
            if df[target_column].dtype in ['object', 'category'] or unique_values <= 10:
                problem_type = 'classification'
            else:
                problem_type = 'regression'

        print(f'Problem type: {problem_type}')

        # Simple data splitting
        df = df.sample(frac=1, random_state=shuffle_seed).reset_index(drop=True)
        train_size = int(len(df) * train_split)
        train_df = df.iloc[:train_size]
        test_df = df.iloc[train_size:]

        print(f'Train set: {train_df.shape}, Test set: {test_df.shape}')

        # Prepare outputs
        feature_columns = [col for col in df.columns if col != target_column]
        dataset_info = {
            'total_samples': len(df),
            'train_samples': len(train_df),
            'test_samples': len(test_df),
            'target_column': target_column,
            'feature_columns': feature_columns,
            'train_split_ratio': train_split,
            'shuffle_seed': shuffle_seed,
            'model_type': model_type,
            'problem_type': problem_type,
            'columns': list(df.columns),
            'dtypes': {col: str(df[col].dtype) for col in df.columns}
        }

        model_config = MODEL_CONFIGS.get(model_type)

        # Save outputs
        os.makedirs(os.path.dirname(train_data_path) or '.', exist_ok=True)
        train_df.to_csv(train_data_path, index=False)

        os.makedirs(os.path.dirname(test_data_path) or '.', exist_ok=True)
        test_df.to_csv(test_data_path, index=False)

        os.makedirs(os.path.dirname(dataset_info_path) or '.', exist_ok=True)
        with open(dataset_info_path, 'wb') as f:
            pickle.dump(dataset_info, f)

        os.makedirs(os.path.dirname(model_config_path) or '.', exist_ok=True)
        with open(model_config_path, 'w') as f:
            json.dump(model_config, f, indent=2)

        print('=== DATA LOADING COMPLETED SUCCESSFULLY ===')
        print(f'Train data saved: {train_data_path}')
        print(f'Test data saved: {test_data_path}')
        print(f'Dataset info saved: {dataset_info_path}')
        print(f'Model config saved: {model_config_path}')
        " "$0" "$1" "$2" "$3" "$4" "$5" "$6" "$7" "$8"
    args:
      - {inputValue: cdn_url}
      - {inputValue: target_column}
      - {inputValue: train_split}
      - {inputValue: shuffle_seed}
      - {inputValue: model_type}
      - {outputPath: train_data}
      - {outputPath: test_data}
      - {outputPath: dataset_info}
      - {outputPath: model_config}
