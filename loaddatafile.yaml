name: 5Generic Load Dataset
description: Loads data and handles different model types automatically
inputs:
  - name: cdn_url
    type: String
    description: 'CDN URL to download CSV file'
  
  - name: target_column
    type: String
    description: 'Target column name for prediction'
  
  - name: train_split
    type: Float
    description: 'Train split ratio'
  
  - name: shuffle_seed
    type: Integer
    description: 'Random seed for shuffling'
  
  - name: model_type
    type: String
    description: 'Type of model to use'

outputs:
  - name: train_data
    type: Dataset
  - name: test_data
    type: Dataset
  - name: dataset_info
    type: DatasetInfo
  - name: model_config
    type: String

implementation:
  container:
    image: python:3.9
    command:
      - sh
      - -c
      - |
        pip install pandas numpy requests scikit-learn >/dev/null 2>&1
        python -c "
        import os
        import sys
        import io
        import pandas as pd
        import numpy as np
        import requests
        import pickle
        import json
        from urllib.parse import unquote
        from sklearn.model_selection import train_test_split

        # Debug: Print all arguments to see what's being passed
        print('=== DEBUG: Arguments received ===')
        for i, arg in enumerate(sys.argv):
            print(f'sys.argv[{i}]: {arg}')

        # Get input arguments - FIXED: Starting from index 1
        # sys.argv[0] is the script name (-c), so inputs start from sys.argv[1]
        cdn_url = sys.argv[1]
        target_column = sys.argv[2]
        train_split = float(sys.argv[3])
        shuffle_seed = int(sys.argv[4])
        model_type = sys.argv[5]
        train_data_path = sys.argv[6]
        test_data_path = sys.argv[7]
        dataset_info_path = sys.argv[8]
        model_config_path = sys.argv[9]

        print('=== Generic Data Loader ===')
        print(f'CDN URL: {cdn_url}')
        print(f'Target column: {target_column}')
        print(f'Train split: {train_split}')
        print(f'Shuffle seed: {shuffle_seed}')
        print(f'Model type: {model_type}')

        # Model configurations for different algorithms
        MODEL_CONFIGS = {
            'linear_regression': {
                'model_type': 'linear_regression',
                'normalize': True,
                'description': 'Linear Regression for regression tasks',
                'problem_type': 'regression',
                'preprocessing': {
                    'handle_missing': 'mean',
                    'encode_categorical': 'onehot'
                }
            },
            'logistic_regression': {
                'model_type': 'logistic_regression',
                'normalize': True,
                'max_iter': 1000,
                'random_state': 42,
                'description': 'Logistic Regression for classification tasks',
                'problem_type': 'classification',
                'preprocessing': {
                    'handle_missing': 'mode',
                    'encode_categorical': 'onehot'
                }
            },
            'random_forest': {
                'model_type': 'random_forest',
                'n_estimators': 100,
                'max_depth': 10,
                'random_state': 42,
                'description': 'Random Forest for robust predictions',
                'problem_type': 'auto',
                'preprocessing': {
                    'handle_missing': 'median',
                    'encode_categorical': 'label'
                }
            }
        }

        # Validate model type
        if model_type not in MODEL_CONFIGS:
            print(f'Warning: Unknown model type {model_type}. Using linear_regression.')
            model_type = 'linear_regression'

        # Download and load data
        print('=== Downloading Data ===')
        decoded_url = unquote(cdn_url)
        try:
            print(f'Downloading from: {decoded_url}')
            response = requests.get(decoded_url, timeout=30)
            response.raise_for_status()
            
            # Try different encodings to handle various CSV formats
            encodings = ['utf-8', 'latin-1', 'iso-8859-1', 'cp1252']
            df = None
            for encoding in encodings:
                try:
                    df = pd.read_csv(io.BytesIO(response.content), encoding=encoding)
                    print(f'Successfully loaded with encoding: {encoding}')
                    break
                except UnicodeDecodeError:
                    continue
            
            if df is None:
                # Fallback with error replacement
                df = pd.read_csv(io.BytesIO(response.content), encoding='utf-8', errors='replace')
                print('Used error replacement for encoding issues')
                
            print(f'Loaded dataset with shape: {df.shape}')
            print(f'Dataset columns: {list(df.columns)}')
            
        except Exception as e:
            print(f'Failed to load dataset: {e}')
            sys.exit(1)

        # Data validation
        print('=== Data Validation ===')
        if target_column not in df.columns:
            print(f'ERROR: Target column {target_column} not found in dataset')
            print(f'Available columns: {list(df.columns)}')
            sys.exit(1)

        # Check for missing values
        missing_values = df.isnull().sum()
        if missing_values.any():
            print('Missing values detected:')
            for col, count in missing_values[missing_values > 0].items():
                print(f'  {col}: {count} ({count/len(df)*100:.1f}%)')
        else:
            print('No missing values detected')

        # Target analysis
        original_target_type = df[target_column].dtype
        unique_values = df[target_column].nunique()
        
        print('=== Target Analysis ===')
        print(f'Original target type: {original_target_type}')
        print(f'Unique values: {unique_values}')
        print(f'Value counts: {df[target_column].value_counts().to_dict()}')

        # Determine problem type based on model and data characteristics
        problem_type = None
        target_encoding = 'direct'
        
        if model_type == 'logistic_regression':
            problem_type = 'classification'
            # For classification, encode categorical targets
            if df[target_column].dtype in ['object', 'category']:
                print('Encoding categorical target for classification')
                df[target_column] = df[target_column].astype('category').cat.codes
                target_encoding = 'label_encoded'
                
        elif model_type == 'linear_regression':
            problem_type = 'regression'
            
        else:  # random_forest - auto-detect based on data
            if df[target_column].dtype in ['object', 'category'] or unique_values <= 10:
                problem_type = 'classification'
                if df[target_column].dtype in ['object', 'category']:
                    print('Encoding categorical target for classification')
                    df[target_column] = df[target_column].astype('category').cat.codes
                    target_encoding = 'label_encoded'
            else:
                problem_type = 'regression'

        MODEL_CONFIGS[model_type]['problem_type'] = problem_type
        print(f'Determined problem type: {problem_type}')

        # Feature analysis
        feature_columns = [col for col in df.columns if col != target_column]
        numeric_features = df[feature_columns].select_dtypes(include=[np.number]).columns.tolist()
        categorical_features = df[feature_columns].select_dtypes(include=['object', 'category']).columns.tolist()

        print('=== Feature Analysis ===')
        print(f'Numeric features ({len(numeric_features)}): {numeric_features}')
        print(f'Categorical features ({len(categorical_features)}): {categorical_features}')

        # Data splitting with stratification for classification
        stratify = None
        if problem_type == 'classification' and unique_values > 1:
            min_class_size = df[target_column].value_counts().min()
            if min_class_size >= 2:
                stratify = df[target_column]
                print('Using stratified sampling for classification')

        # Adjust train_split if needed
        if train_split > 1.0:
            print(f'Warning: train_split {train_split} is greater than 1.0, converting to ratio')
            train_split = train_split / 100.0

        train_split = max(0.1, min(0.9, train_split))
        
        print('=== Splitting Data ===')
        train_df, test_df = train_test_split(
            df, 
            train_size=train_split, 
            random_state=shuffle_seed,
            stratify=stratify
        )

        print(f'Train set: {train_df.shape}, Test set: {test_df.shape}')
        if problem_type == 'classification':
            print(f'Train class distribution: {train_df[target_column].value_counts().to_dict()}')
            print(f'Test class distribution: {test_df[target_column].value_counts().to_dict()}')

        # Prepare dataset info
        dataset_info = {
            'metadata': {
                'total_samples': len(df),
                'train_samples': len(train_df),
                'test_samples': len(test_df),
                'target_column': target_column,
                'feature_columns': feature_columns,
                'numeric_features': numeric_features,
                'categorical_features': categorical_features,
                'train_split_ratio': train_split,
                'shuffle_seed': shuffle_seed,
                'model_type': model_type,
                'target_encoding': target_encoding,
                'original_target_type': str(original_target_type),
                'problem_type': problem_type,
                'columns': list(df.columns),
                'dtypes': {col: str(df[col].dtype) for col in df.columns},
                'target_distribution': df[target_column].value_counts().to_dict(),
                'missing_values': missing_values[missing_values > 0].to_dict()
            }
        }

        # Get model configuration
        model_config = MODEL_CONFIGS.get(model_type)

        # Save outputs
        print('=== Saving Outputs ===')
        
        # Create directories and save files
        for path in [train_data_path, test_data_path, dataset_info_path, model_config_path]:
            dir_path = os.path.dirname(path)
            if dir_path:
                os.makedirs(dir_path, exist_ok=True)

        train_df.to_csv(train_data_path, index=False)
        print(f'Saved train data: {train_data_path}')

        test_df.to_csv(test_data_path, index=False)
        print(f'Saved test data: {test_data_path}')

        with open(dataset_info_path, 'wb') as f:
            pickle.dump(dataset_info, f)
        print(f'Saved dataset info: {dataset_info_path}')

        with open(model_config_path, 'w') as f:
            json.dump(model_config, f, indent=2)
        print(f'Saved model config: {model_config_path}')

        print('=== Data Loading Complete ===')
        print(f'Final problem type: {problem_type}')
        print(f'Target encoding: {target_encoding}')
        print(f'Train samples: {len(train_df)}')
        print(f'Test samples: {len(test_df)}')
        print('All outputs saved successfully!')
        " "$1" "$2" "$3" "$4" "$5" "$6" "$7" "$8" "$9"
    args:
      - {inputValue: cdn_url}
      - {inputValue: target_column}
      - {inputValue: train_split}
      - {inputValue: shuffle_seed}
      - {inputValue: model_type}
      - {outputPath: train_data}
      - {outputPath: test_data}
      - {outputPath: dataset_info}
      - {outputPath: model_config}
