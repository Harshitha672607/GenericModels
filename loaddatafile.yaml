name: Genericloaddataset123
description: Loads data and handles different model types automatically
inputs:
  - name: cdn_url
    type: String
    description: 'CDN URL to download CSV file'
    default: 'https://cdn.gov-cloud.ai//_public/pipeline/fafc3d9b-eb56-4990-b277-5ff8e247e693_%24%24_V1_hr_comp_benchmarking.csv'
  
  - name: target_column
    type: String
    description: 'Target column name for prediction'
    default: 'salary'
  
  - name: train_split
    type: Float
    description: 'Train split ratio'
    default: 0.75
  
  - name: shuffle_seed
    type: Integer
    description: 'Random seed for shuffling'
    default: 42
  
  - name: model_type
    type: String
    description: 'Type of model to use'
    default: 'linear_regression'


outputs:
  - name: train_data
    type: Dataset
  
  - name: test_data
    type: Dataset
  
  - name: dataset_info
    type: DatasetInfo
  
  - name: model_config
    type: String

implementation:
  container:
    image: python:3.9
    command:
      - sh
      - -c
      - |
        pip install pandas numpy requests scikit-learn >/dev/null 2>&1
        python -c "
        import os
        import sys
        import io
        import pandas as pd
        import numpy as np
        import requests
        import pickle
        import json
        from urllib.parse import unquote
        from sklearn.model_selection import train_test_split

        # Get input arguments
        cdn_url = sys.argv[1]
        target_column = sys.argv[2]
        train_split = float(sys.argv[3])
        shuffle_seed = int(sys.argv[4])
        model_type = sys.argv[5]
        train_data_path = sys.argv[6]
        test_data_path = sys.argv[7]
        dataset_info_path = sys.argv[8]
        model_config_path = sys.argv[9]

        print('GENERIC DATA LOADER')
        print(f'Model Type: {model_type}')
        print(f'Target Column: {target_column}')
        print(f'Train Split: {train_split}')
        print(f'Shuffle Seed: {shuffle_seed}')

        # Model-specific configurations
        MODEL_CONFIGS = {
            'linear_regression': {
                'model_type': 'linear_regression',
                'transformation': 'log_target',
                'normalize': True,
                'description': 'Linear Regression with log transformation for salary prediction',
                'problem_type': 'regression'
            },
            'logistic_regression': {
                'model_type': 'logistic_regression',
                'normalize': True,
                'max_iter': 1000,
                'random_state': 42,
                'description': 'Logistic Regression for classification tasks',
                'problem_type': 'classification'
            },
            'random_forest': {
                'model_type': 'random_forest',
                'n_estimators': 100,
                'max_depth': 10,
                'random_state': 42,
                'description': 'Random Forest for robust predictions',
                'problem_type': 'auto'  # Will be determined from data
            }
        }

        # Download and load data
        decoded_url = unquote(cdn_url)
        try:
            print('Downloading dataset')
            response = requests.get(decoded_url, timeout=30)
            response.raise_for_status()
            df = pd.read_csv(io.BytesIO(response.content))
            print(f'Loaded dataset with shape: {df.shape}')
            print(f'Columns: {list(df.columns)}')
        except Exception as e:
            print(f'Failed to load dataset: {e}')
            sys.exit(1)

        # Data validation
        if target_column not in df.columns:
            print(f'Target column {target_column} not found in dataset')
            print(f'Available columns: {list(df.columns)}')
            sys.exit(1)

        # Handle target variable based on model type
        original_target_type = df[target_column].dtype
        print(f'Original target type: {original_target_type}')
        print(f'Unique target values: {df[target_column].nunique()}')

        if model_type == 'logistic_regression':
            # For classification, encode categorical targets
            if df[target_column].dtype == 'object' or df[target_column].nunique() <= 10:
                print('Encoding target for classification')
                df[target_column] = df[target_column].astype('category').cat.codes
                target_encoding = 'encoded'
            else:
                print('Warning: Many unique values for classification. Consider binning.')
                target_encoding = 'direct'
        else:
            target_encoding = 'direct'

        # Determine problem type for random forest
        if model_type == 'random_forest':
            if df[target_column].nunique() <= 10 or df[target_column].dtype == 'object':
                problem_type = 'classification'
                MODEL_CONFIGS['random_forest']['problem_type'] = 'classification'
            else:
                problem_type = 'regression'
                MODEL_CONFIGS['random_forest']['problem_type'] = 'regression'
            print(f'Random Forest problem type: {problem_type}')

        # Split data with stratification for classification
        stratify = None
        if model_type == 'logistic_regression' or (model_type == 'random_forest' and problem_type == 'classification'):
            stratify = df[target_column]
            print('Using stratified sampling for classification')

        train_df, test_df = train_test_split(
            df, 
            train_size=train_split, 
            random_state=shuffle_seed,
            stratify=stratify
        )

        print(f'Train shape: {train_df.shape}, Test shape: {test_df.shape}')

        # Feature analysis
        feature_columns = [col for col in df.columns if col != target_column]
        numeric_features = df[feature_columns].select_dtypes(include=[np.number]).columns.tolist()
        categorical_features = df[feature_columns].select_dtypes(include=['object', 'category']).columns.tolist()

        print(f'Numeric features ({len(numeric_features)}): {numeric_features}')
        print(f'Categorical features ({len(categorical_features)}): {categorical_features}')

        # Prepare dataset info
        dataset_info = {
            'metadata': {
                'total_samples': len(df),
                'train_samples': len(train_df),
                'test_samples': len(test_df),
                'target_column': target_column,
                'feature_columns': feature_columns,
                'numeric_features': numeric_features,
                'categorical_features': categorical_features,
                'train_split_ratio': train_split,
                'shuffle_seed': shuffle_seed,
                'model_type': model_type,
                'target_encoding': target_encoding,
                'original_target_type': str(original_target_type)
            },
            'target_analysis': {
                'n_unique': int(df[target_column].nunique()),
                'value_counts': df[target_column].value_counts().to_dict(),
                'target_mean': float(df[target_column].mean()) if df[target_column].dtype in [np.number] else None,
                'target_std': float(df[target_column].std()) if df[target_column].dtype in [np.number] else None
            },
            'feature_analysis': {
                'total_features': len(feature_columns),
                'numeric_count': len(numeric_features),
                'categorical_count': len(categorical_features),
                'columns': list(df.columns),
                'dtypes': {col: str(df[col].dtype) for col in df.columns}
            }
        }

        # Set problem type in dataset info
        if model_type == 'logistic_regression':
            dataset_info['metadata']['problem_type'] = 'classification'
        elif model_type == 'linear_regression':
            dataset_info['metadata']['problem_type'] = 'regression'
        else:
            dataset_info['metadata']['problem_type'] = problem_type

        # Get model configuration
        model_config = MODEL_CONFIGS.get(model_type, MODEL_CONFIGS['linear_regression'])
        model_config['problem_type'] = dataset_info['metadata']['problem_type']

        # Save outputs
        os.makedirs(os.path.dirname(train_data_path) or '.', exist_ok=True)
        train_df.to_csv(train_data_path, index=False)

        os.makedirs(os.path.dirname(test_data_path) or '.', exist_ok=True)
        test_df.to_csv(test_data_path, index=False)

        os.makedirs(os.path.dirname(dataset_info_path) or '.', exist_ok=True)
        with open(dataset_info_path, 'wb') as f:
            pickle.dump(dataset_info, f)

        os.makedirs(os.path.dirname(model_config_path) or '.', exist_ok=True)
        with open(model_config_path, 'w') as f:
            json.dump(model_config, f, indent=2)

        print('Data loading complete!')
        print(f'Train data: {train_data_path}')
        print(f'Test data: {test_data_path}')
        print(f'Dataset info: {dataset_info_path}')
        print(f'Model config: {model_config_path}')
        print(f'Problem type: {dataset_info[\\\"metadata\\\"][\\\"problem_type\\\"]}')
        " "$0" "$1" "$2" "$3" "$4" "$5" "$6" "$7" "$8"
    args:
      - {inputValue: cdn_url}
      - {inputValue: target_column}
      - {inputValue: train_split}
      - {inputValue: shuffle_seed}
      - {inputValue: model_type}
      - {outputPath: train_data}
      - {outputPath: test_data}
      - {outputPath: dataset_info}
      - {outputPath: model_config}
