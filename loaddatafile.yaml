name: 1Generic Data Loader
description: Downloads a CSV file from a CDN, splits it into train/test datasets, and generates dataset metadata for multiple model types (Linear Regression, Logistic Regression, Random Forest)

inputs:
  - {name: cdn_url, type: String, description: 'CDN URL to download CSV file'}
  - {name: target_column, type: String, description: 'Target column name'}
  - {name: model_config, type: String, description: 'Model configuration as JSON string containing model_type and parameters'}
  - {name: train_split, type: Float, description: 'Train split ratio (default 0.75)'}
  - {name: shuffle_seed, type: Integer, description: 'Random seed for shuffling'}

outputs:
  - {name: train_data, type: Dataset}
  - {name: test_data, type: Dataset}
  - {name: dataset_info, type: DatasetInfo}

implementation:
  container:
    image: python:3.9
    command:
      - sh
      - -c
      - |
        pip install pandas numpy requests scikit-learn >/dev/null 2>&1
        python -c "
        import os
        import sys
        import io
        import json
        import pandas as pd
        import numpy as np
        import requests
        import pickle
        from urllib.parse import unquote
        from sklearn.model_selection import train_test_split

        cdn_url = sys.argv[1]
        target_column = sys.argv[2]
        model_config_str = sys.argv[3]
        train_split = float(sys.argv[4])
        shuffle_seed = int(sys.argv[5])
        train_data_path = sys.argv[6]
        test_data_path = sys.argv[7]
        dataset_info_path = sys.argv[8]

        print('Generic CSV Loader - Processing Model Configuration')

        # Parse model configuration to extract model_type
        try:
            model_config = json.loads(model_config_str)
            model_type = model_config.get('model_type', 'random_forest')
            print(f'Successfully parsed model configuration: {model_type}')
            print(f'Full model config: {model_config}')
        except Exception as e:
            print(f'Error parsing model config: {e}')
            print('Using default model: random_forest')
            model_type = 'random_forest'
            model_config = {'model_type': 'random_forest'}

        print(f'Model Type: {model_type}')
        print(f'CDN URL: {cdn_url}')
        print(f'Target column: {target_column}')
        print(f'Train split: {train_split}')
        print(f'Shuffle seed: {shuffle_seed}')

        # Validate train_split parameter
        if train_split > 1.0:
            print(f'Warning: train_split {train_split} is greater than 1.0, converting to ratio')
            train_split = train_split / 100.0
            print(f'Converted train_split to: {train_split}')

        decoded_url = unquote(cdn_url)
        try:
            print('Downloading CSV')
            response = requests.get(decoded_url, timeout=30)
            response.raise_for_status()
            df = pd.read_csv(io.BytesIO(response.content))
            print(f'Loaded CSV with shape: {df.shape}')
            print(f'Columns: {list(df.columns)}')
        except Exception as e:
            print(f'Failed to load dataset: {e}')
            raise

        # Determine problem type based on model_type
        if model_type == 'linear_regression':
            problem_type = 'regression'
        else:
            problem_type = 'classification'

        print(f'Problem Type: {problem_type}')

        # Check target variable
        target_unique = df[target_column].nunique()
        target_distribution = df[target_column].value_counts()
        print(f'Target variable has {target_unique} unique values: {target_distribution.to_dict()}')

        # Basic data validation
        print('Data Summary:')
        print(df.describe())
        print('Missing values:')
        print(df.isnull().sum())

        # Ensure train_split is between 0 and 1
        train_split = max(0.1, min(0.9, train_split))
        test_size = 1.0 - train_split
        
        print(f'Using train split: {train_split}, test size: {test_size}')

        # Use stratified split for classification if possible
        if problem_type == 'classification':
            min_samples_per_class = df[target_column].value_counts().min()
            can_stratify = min_samples_per_class >= 2
            
            if can_stratify:
                print('Using stratified split (enough samples in all classes)')
                train_df, test_df = train_test_split(
                    df, 
                    test_size=test_size, 
                    random_state=shuffle_seed,
                    stratify=df[target_column]
                )
            else:
                print(f'Warning: Cannot use stratified split. Minimum samples per class: {min_samples_per_class}')
                print('Using random split instead')
                train_df, test_df = train_test_split(
                    df, 
                    test_size=test_size, 
                    random_state=shuffle_seed
                )
        else:
            # Regression - simple split
            print('Using random split for regression')
            train_df, test_df = train_test_split(
                df, 
                test_size=test_size, 
                random_state=shuffle_seed
            )

        print(f'Train shape: {train_df.shape}, Test shape: {test_df.shape}')
        if problem_type == 'classification':
            print(f'Class distribution in train: {train_df[target_column].value_counts().to_dict()}')
            print(f'Class distribution in test: {test_df[target_column].value_counts().to_dict()}')

        feature_columns = [col for col in df.columns if col != target_column and col != 'employee_id']
        
        # Auto-detect feature types
        numeric_features = []
        categorical_features = []
        for col in feature_columns:
            if col in df.columns:
                if df[col].dtype in ['int64', 'float64']:
                    numeric_features.append(col)
                else:
                    categorical_features.append(col)
        
        dataset_info = {
            'total_samples': len(df),
            'train_samples': len(train_df),
            'test_samples': len(test_df),
            'target_column': target_column,
            'feature_columns': feature_columns,
            'numeric_features': numeric_features,
            'categorical_features': categorical_features,
            'train_split_ratio': train_split,
            'shuffle_seed': shuffle_seed,
            'columns': list(df.columns),
            'dtypes': {col: str(df[col].dtype) for col in df.columns},
            'target_distribution': df[target_column].value_counts().to_dict() if problem_type == 'classification' else {},
            'problem_type': problem_type,
            'model_type': model_type,
            'model_config': model_config,
            'dataset_name': 'employee_performance',
            'used_stratified_split': can_stratify if problem_type == 'classification' else False
        }

        # Create directories safely
        def ensure_directory(path):
            directory = os.path.dirname(path)
            if directory and not os.path.exists(directory):
                os.makedirs(directory, exist_ok=True)
        
        # Save files with proper directory creation
        ensure_directory(train_data_path)
        train_df.to_csv(train_data_path, index=False)

        ensure_directory(test_data_path)
        test_df.to_csv(test_data_path, index=False)

        ensure_directory(dataset_info_path)
        with open(dataset_info_path, 'wb') as f:
            pickle.dump(dataset_info, f)

        print('Dataset processing complete!')
        print(f'Train data saved at: {train_data_path}')
        print(f'Test data saved at: {test_data_path}')
        print(f'Dataset info saved at: {dataset_info_path}')
        print(f'Dataset info keys: {list(dataset_info.keys())}')
        " "$0" "$1" "$2" "$3" "$4" "$5" "$6" "$7"
    args:
      - {inputValue: cdn_url}
      - {inputValue: target_column}
      - {inputValue: model_config}
      - {inputValue: train_split}
      - {inputValue: shuffle_seed}
      - {outputPath: train_data}
      - {outputPath: test_data}
      - {outputPath: dataset_info}
