name: Generic Model Evaluation
description: Generic evaluation component for regression and classification models
inputs:
  - name: trained_model
    type: Model
  - name: processed_data
    type: Dataset
  - name: preprocessing_pipeline
    type: Model
  - name: training_history
    type: String
  - name: model_config
    type: String
outputs:
  - name: evaluation_metrics
    type: Metrics
  - name: evaluation_report
    type: String
  - name: visualization_data
    type: String
implementation:
  container:
    image: python:3.9
    command:
      - sh
      - -c
      - |
        pip install pandas numpy scikit-learn matplotlib seaborn >/dev/null 2>&1
        python -c "
        import sys, os, pickle, json, pandas as pd, numpy as np
        import matplotlib.pyplot as plt
        import seaborn as sns
        from sklearn.metrics import (
            r2_score, mean_absolute_error, mean_squared_error, 
            accuracy_score, precision_score, recall_score, f1_score,
            classification_report, confusion_matrix, roc_curve, auc
        )
        from io import StringIO

        # Get arguments
        trained_model_path = sys.argv[1]
        processed_data_path = sys.argv[2]
        preprocessing_pipeline_path = sys.argv[3]
        training_history_path = sys.argv[4]
        model_config_path = sys.argv[5]
        evaluation_metrics_path = sys.argv[6]
        evaluation_report_path = sys.argv[7]
        visualization_data_path = sys.argv[8]

        print('Generic Model Evaluation Started')
        
        # Load all artifacts
        with open(trained_model_path, 'rb') as f:
            model = pickle.load(f)
            
        with open(processed_data_path, 'rb') as f:
            processed_data = pickle.load(f)
            
        with open(preprocessing_pipeline_path, 'rb') as f:
            preprocessor = pickle.load(f)
            
        with open(training_history_path, 'r') as f:
            training_history = json.load(f)
            
        with open(model_config_path, 'r') as f:
            model_config = json.load(f)
        
        # Extract test data
        X_test = processed_data['X_test']
        y_test = processed_data['y_test']
        X_test_processed = processed_data['X_test_processed']
        
        model_type = model_config['model_type']
        problem_type = model_config['problem_type']
        
        print(f'Evaluating {model_type} for {problem_type} problem')
        print(f'Test samples: {len(X_test)}')
        
        # Make predictions
        y_pred = model.predict(X_test_processed)
        
        # Calculate comprehensive metrics based on problem type
        if problem_type == 'regression':
            # Regression metrics
            r2 = r2_score(y_test, y_pred)
            mae = mean_absolute_error(y_test, y_pred)
            rmse = np.sqrt(mean_squared_error(y_test, y_pred))
            mse = mean_squared_error(y_test, y_pred)
            
            # Additional regression metrics
            mape = np.mean(np.abs((y_test - y_pred) / np.where(y_test == 0, 1, y_test))) * 100
            explained_variance = 1 - (np.var(y_test - y_pred) / np.var(y_test))
            
            # Error distribution
            errors = y_test - y_pred
            error_stats = {
                'mean_error': float(np.mean(errors)),
                'std_error': float(np.std(errors)),
                'median_error': float(np.median(errors)),
                'max_error': float(np.max(np.abs(errors)))
            }
            
            metrics = {
                'r2_score': float(r2),
                'mae': float(mae),
                'rmse': float(rmse),
                'mse': float(mse),
                'mape': float(mape),
                'explained_variance': float(explained_variance),
                'error_statistics': error_stats
            }
            
            # Target statistics
            target_stats = {
                'actual_mean': float(np.mean(y_test)),
                'actual_std': float(np.std(y_test)),
                'predicted_mean': float(np.mean(y_pred)),
                'predicted_std': float(np.std(y_pred)),
                'actual_min': float(np.min(y_test)),
                'actual_max': float(np.max(y_test)),
                'predicted_min': float(np.min(y_pred)),
                'predicted_max': float(np.max(y_pred))
            }
            
        else:
            # Classification metrics
            accuracy = accuracy_score(y_test, y_pred)
            precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
            recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
            f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)
            
            # Detailed classification report
            class_report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)
            
            # Confusion matrix
            cm = confusion_matrix(y_test, y_pred)
            
            metrics = {
                'accuracy': float(accuracy),
                'precision': float(precision),
                'recall': float(recall),
                'f1_score': float(f1),
                'classification_report': class_report,
                'confusion_matrix': cm.tolist()
            }
            
            # For binary classification, calculate ROC AUC if possible
            if len(np.unique(y_test)) == 2 and hasattr(model, 'predict_proba'):
                y_prob = model.predict_proba(X_test_processed)[:, 1]
                fpr, tpr, _ = roc_curve(y_test, y_prob)
                roc_auc = auc(fpr, tpr)
                metrics['roc_auc'] = float(roc_auc)
                metrics['fpr'] = fpr.tolist()
                metrics['tpr'] = tpr.tolist()
            
            target_stats = {
                'class_distribution': dict(zip(*np.unique(y_test, return_counts=True))),
                'predicted_distribution': dict(zip(*np.unique(y_pred, return_counts=True)))
            }
        
        # Create comprehensive evaluation metrics
        evaluation_metrics = {
            'model_type': model_type,
            'problem_type': problem_type,
            'test_samples': len(X_test),
            'metrics': metrics,
            'target_statistics': target_stats,
            'model_config': model_config,
            'evaluation_timestamp': pd.Timestamp.now().isoformat()
        }
        
        # Create detailed evaluation report
        report_lines = []
        report_lines.append('GENERIC MODEL EVALUATION REPORT')
        report_lines.append('=' * 50)
        report_lines.append(f'Model Type: {model_type}')
        report_lines.append(f'Problem Type: {problem_type}')
        report_lines.append(f'Test Samples: {len(X_test)}')
        report_lines.append('')
        report_lines.append('PERFORMANCE METRICS:')
        report_lines.append('-' * 30)
        
        if problem_type == 'regression':
            report_lines.append(f'R² Score: {metrics[\\\"r2_score\\\"]:.3f}')
            report_lines.append(f'Mean Absolute Error: {metrics[\\\"mae\\\"]:.3f}')
            report_lines.append(f'Root Mean Squared Error: {metrics[\\\"rmse\\\"]:.3f}')
            report_lines.append(f'Mean Absolute Percentage Error: {metrics[\\\"mape\\\"]:.1f}%')
            report_lines.append(f'Explained Variance: {metrics[\\\"explained_variance\\\"]:.3f}')
        else:
            report_lines.append(f'Accuracy: {metrics[\\\"accuracy\\\"]:.3f}')
            report_lines.append(f'Precision: {metrics[\\\"precision\\\"]:.3f}')
            report_lines.append(f'Recall: {metrics[\\\"recall\\\"]:.3f}')
            report_lines.append(f'F1 Score: {metrics[\\\"f1_score\\\"]:.3f}')
            if 'roc_auc' in metrics:
                report_lines.append(f'ROC AUC: {metrics[\\\"roc_auc\\\"]:.3f}')
        
        report_lines.append('')
        report_lines.append('TARGET STATISTICS:')
        report_lines.append('-' * 30)
        for key, value in target_stats.items():
            if isinstance(value, dict):
                report_lines.append(f'{key}:')
                for k, v in value.items():
                    report_lines.append(f'  {k}: {v}')
            else:
                report_lines.append(f'{key}: {value:.3f}')
        
        evaluation_report = '\\n'.join(report_lines)
        
        # Create visualization data
        visualization_data = {
            'model_type': model_type,
            'problem_type': problem_type,
            'y_true': y_test.tolist() if hasattr(y_test, 'tolist') else list(y_test),
            'y_pred': y_pred.tolist() if hasattr(y_pred, 'tolist') else list(y_pred),
            'metrics': metrics,
            'feature_names': preprocessor.get_feature_names_out().tolist()
        }
        
        # Save outputs
        os.makedirs(os.path.dirname(evaluation_metrics_path) or '.', exist_ok=True)
        with open(evaluation_metrics_path, 'w') as f:
            json.dump(evaluation_metrics, f, indent=2)
            
        os.makedirs(os.path.dirname(evaluation_report_path) or '.', exist_ok=True)
        with open(evaluation_report_path, 'w') as f:
            f.write(evaluation_report)
            
        os.makedirs(os.path.dirname(visualization_data_path) or '.', exist_ok=True)
        with open(visualization_data_path, 'w') as f:
            json.dump(visualization_data, f, indent=2)
        
        print('Generic Model Evaluation Completed Successfully!')
        print('Key Metrics:')
        if problem_type == 'regression':
            print(f'  R²: {metrics[\\\"r2_score\\\"]:.3f}, RMSE: {metrics[\\\"rmse\\\"]:.3f}')
        else:
            print(f'  Accuracy: {metrics[\\\"accuracy\\\"]:.3f}, F1: {metrics[\\\"f1_score\\\"]:.3f}')
        " "$0" "$1" "$2" "$3" "$4" "$5" "$6" "$7" "$8"
    args:
      - {inputPath: trained_model}
      - {inputPath: processed_data}
      - {inputPath: preprocessing_pipeline}
      - {inputPath: training_history}
      - {inputValue: model_config}
      - {outputPath: evaluation_metrics}
      - {outputPath: evaluation_report}
      - {outputPath: visualization_data}
